{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Scrape and Save reddit data to folders\n",
    "        edit your credentials in ScrapeRedditData.py before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.2.0 was released Wednesday February 24, 2021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMU:100\n",
      "drexel:100\n",
      "temple:73\n",
      "upenn:100\n",
      "villanova:100\n"
     ]
    }
   ],
   "source": [
    "from ScrapeRedditData import scrapeSubreddit\n",
    "\n",
    "university_subreddits = [ \"CMU\", \"drexel\", \"temple\", \"upenn\", \"villanova\"]\n",
    "nonuniversity_subreddits = [ \"investing\", \"AskNYC\", \"thingsmykidsaid\", \"nfl\", \"help\"]\n",
    "\n",
    "for university_subreddit in university_subreddits:\n",
    "    scrapeSubreddit(subreddit_name=university_subreddit, file_directory='./data/university_data/')\n",
    "\n",
    "for nonuniversity_subreddit in nonuniversity_subreddits:\n",
    "    scrapeSubreddit(subreddit_name=nonuniversity_subreddit, file_directory='./data/nonuniversity_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Collect post meta data and parse through post 'title', 'user', and 'text'\n",
    "        text files are line separated, with first two lines being the title and user name - \"title\\n user\\n text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonUniFiles=os.listdir('./data/nonuniversity_data/')\n",
    "uniFiles=os.listdir('./data/university_data/')\n",
    "\n",
    "df_dict = {'titles':[], 'user':[], 'text':[], 'uni_label':[], 'reddit_label':[]}\n",
    "\n",
    "# non-University\n",
    "for nonUFile in nonUniFiles:\n",
    "    \n",
    "    # Handle dirty file contents\n",
    "    if nonUFile == '.DS_Store' or nonUFile == '.ipynb_checkpoints':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        # Subreddit name is the prefix of file name\n",
    "        df_dict['reddit_label'].append(re.findall('(\\w+)_',nonUFile)[0])\n",
    "        df_dict['uni_label'].append('non_uni')\n",
    "\n",
    "        with open('./data/nonuniversity_data/'+nonUFile, 'r') as f:\n",
    "            p=f.readlines()\n",
    "            df_dict['titles'].append(p[0].strip())\n",
    "            df_dict['user'].append(p[1].strip())\n",
    "            df_dict['text'].append(''.join(p[2:]).replace('\\n', ' '))\n",
    "\n",
    "# University\n",
    "for uFile in uniFiles:\n",
    "    \n",
    "    # Handle dirty file contents\n",
    "    if uFile == '.DS_Store' or nonUFile == '.ipynb_checkpoints':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        # Subreddit name is the prefix of file name\n",
    "        df_dict['reddit_label'].append(re.findall('(\\w+)_',uFile)[0])\n",
    "        df_dict['uni_label'].append('uni')\n",
    "\n",
    "        with open('./data/university_data/'+uFile, 'r') as f:\n",
    "            p=f.readlines()\n",
    "            df_dict['titles'].append(p[0].strip())\n",
    "            df_dict['user'].append(p[1].strip())\n",
    "            df_dict['text'].append(''.join(p[2:]).replace('\\n', ' '))\n",
    "            \n",
    "            \n",
    "df_LARGE = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Convert text and titles to TF-IDF vectors and save to csv for modeling\n",
    "        Also save the raw text and label data in separate CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "tf_idf_text = tf_idf_vectorizer.fit_transform(df_LARGE['text']).toarray()\n",
    "tf_idf_titles = tf_idf_vectorizer.fit_transform(df_LARGE['titles']).toarray()\n",
    "\n",
    "np.savetxt('./data/tf_idf_text.csv', X=tf_idf_text, delimiter=',')\n",
    "np.savetxt('./data/tf_idf_titles.csv', X=tf_idf_titles, delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "df_LARGE[['uni_label', 'reddit_label']].to_csv('./data/y_data.csv', index=False)\n",
    "df_LARGE[['text']].to_csv('./data/raw_text.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
